{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add bias term\n",
    "def addBias(data):\n",
    "    data = np.insert(data, data.shape[0], 1, axis=0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "mndata = MNIST('../mnist')\n",
    "trainData, trainLabel = mndata.load_training()\n",
    "testData, testLabel = mndata.load_testing()\n",
    "\n",
    "\n",
    "\n",
    "trainData = np.array(trainData).T / 255.\n",
    "trainLabel = np.array(trainLabel).T\n",
    "testData = np.array(testData).T / 255.\n",
    "testLabel = np.array(testLabel).T\n",
    "\n",
    "# zero mean and unit variance\n",
    "trainData -= np.mean(trainData, axis=0)\n",
    "trainData /= np.std(trainData, axis = 0)\n",
    "\n",
    "testData -= np.mean(testData, axis=0)\n",
    "testData /= np.std(testData, axis = 0)\n",
    "\n",
    "testData = addBias(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# activation function\n",
    "#1. for hidden layer\n",
    "def ReLU(X,W):\n",
    "    a = np.dot(W.T, X)\n",
    "    return a * (a > 0)\n",
    "#2. for output layer\n",
    "def softMax(X, W):\n",
    "    ak = np.dot(W.T, X)\n",
    "    scores = np.exp(ak)\n",
    "    return scores / np.sum(scores, axis=0, keepdims=True)\n",
    "\n",
    "def sigmoid(X, W):\n",
    "    a = np.dot(W.T, X)\n",
    "    return 1.0 / (1 + np.exp(-1.0 * a))\n",
    "\n",
    "def tanh(X, W):\n",
    "    a = np.dot(W.T, X)\n",
    "    return 1.7159 * np.tanh((2/3) * a)\n",
    "\n",
    "# randomly shuffle the data\n",
    "def shuffle(X, t):\n",
    "    ind = np.random.permutation(t.size)\n",
    "    X, t = X[:, ind], t[ind]\n",
    "    return X, t\n",
    "\n",
    "def plotLoss(Tar):\n",
    "    loss = Tar.loss\n",
    "    iteration = loss.shape[1]\n",
    "    epacho = iteration / Tar.miniBatch\n",
    "    xx = np.linspace(1, epacho, iteration)\n",
    "    Flag = Tar.earlyStop\n",
    "    plt.figure(size = (8, 6))\n",
    "    plt.plot(xx, loss[0], label = 'train loss')\n",
    "    plt.plot(xx, loss[-1], label = 'test loss')\n",
    "    if loss.shape[0] == 3:\n",
    "        plt.plot(xx, loss[1], label = 'validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "def plotError(Tar):\n",
    "    error = Tar.error\n",
    "    iteration = loss.shape[1]\n",
    "    epacho = iteration / Tar.miniBatch\n",
    "    xx = np.linspace(1, epacho, iteration)\n",
    "    Flag = Tar.earlyStop\n",
    "    plt.figure(size = (8, 6))\n",
    "    plt.plot(xx, loss[0], label = 'train percent correct')\n",
    "    plt.plot(xx, loss[-1], label = 'test percent correct')\n",
    "    if Flag:\n",
    "        plt.plot(xx,error[1], label = 'validation percent correct')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class multiLayer():\n",
    "    def __init__(self, trainData, trainLabel, nHiddenLayer = (64,), lr = 1e-3, anneal = 0., maxIter = 1000, earlyStop=3, miniBatch = 1, lam = 0., momentum = 0.9, actFuns=(\"softMax\",\"softMax\")):\n",
    "        self.X = trainData\n",
    "        self.t = trainLabel\n",
    "        self.X = addBias(self.X)\n",
    "        self.earlyStop = earlyStop\n",
    "        self.nClass = len(np.unique(trainLabel))\n",
    "        if earlyStop:\n",
    "            validSize = self.X.shape[1] / 6\n",
    "            self.xValid = self.X[:,:validSize]\n",
    "            self.tValid = self.t[:validSize]\n",
    "            self.X = self.X[:,validSize:]\n",
    "            self.t = self.t[validSize:]\n",
    "        self.actFuns = actFuns\n",
    "        self.lam = lam\n",
    "        self.miniBatch = miniBatch\n",
    "        self.loss = np.array([[], [], []]) if earlyStop else np.array([[], []])\n",
    "        self.error = np.array([[], [], []]) if earlyStop else np.array([[], []])\n",
    "        nLayer = len(nHiddenLayer) + 1\n",
    "        self.numLayer = nLayer\n",
    "        nUnits = (self.X.shape[0],) + nHiddenLayer + (self.nClass,)\n",
    "        self.W = [1 / np.sqrt(nUnits[i]) * np.random.randn(nUnits[i] + 1, nUnits[i]) for i in range(nLayer)]\n",
    "        self.initW = self.W\n",
    "        self.W = self.train(self.W, lr, maxIter, anneal, earlyStop, miniBatch, momentum)\n",
    "\n",
    "    def train(self, weights, lr, maxIter, anneal, earlyStop, miniBatch, mu):\n",
    "        d,n = self.X.shape\n",
    "        batchSize = n / miniBatch\n",
    "        self.wRecords = [[] for _ in weights]\n",
    "        v = [0 for _ in weights]\n",
    "        trainLoss = trainError = validLoss = validError = testLoss = testError = 0\n",
    "        it = 0\n",
    "        while it < maxIter:\n",
    "            startInd , endInd = 0, batchSize\n",
    "            self.X, self.t = shuffle(self.X, self.t)\n",
    "            for i in range(miniBatch):\n",
    "                xBatch, tBatch = self.X[:, startInd:endInd], self.t[startInd:endInd]\n",
    "\n",
    "                if anneal:\n",
    "                    if it * miniBatch + i > 5:\n",
    "                        lr1 = lr / (1. + it / anneal)\n",
    "                    else:\n",
    "                        lr1 = 1e0\n",
    "                else:\n",
    "                    lr1 = lr\n",
    "\n",
    "                outputs = self.forwardProp(xBatch, weights)\n",
    "                dW = self.backProp(xBatch, tBatch, outputs, weights)\n",
    "                trainLoss, trainError = self.evalLossError(xBatch, tBatch, weights)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    testLoss, testError = self.evalTest(testData, testLabel, weights)\n",
    "\n",
    "                for j in range(len(self.wRecords)):\n",
    "                    self.wRecords[j].append(np.array(weights[j]))\n",
    "\n",
    "                if earlyStop:\n",
    "                    self.loss = np.hstack([self.loss, [[trainLoss], [validLoss], [testLoss]]])\n",
    "                    self.error = np.hstack([self.error, [[trainError], [validError], [testError]]])\n",
    "                    if i % 100 == 0:\n",
    "                        validLoss, validError = self.evalLossError(self.xValid, self.tValid, weights)\n",
    "                else:\n",
    "                    self.loss = np.hstack([self.loss, [[trainLoss], [testLoss]]])\n",
    "                    self.error = np.hstack([self.error, [[trainError], [testError]]])\n",
    "\n",
    "                for j in range(len(weights)):\n",
    "                    vPrev = v[j]\n",
    "                    v[j] = mu * v[j] - lr * dW[j]\n",
    "                    weights[j] += -mu * vPrev + (1 + mu) * v[j]\n",
    "\n",
    "                startInd, endInd = startInd + batchSize, endInd + batchSize if i != miniBatch - 1 else n\n",
    "\n",
    "            if earlyStop:\n",
    "                print (trainLoss, trainError, validLoss, validError, testLoss, testError)\n",
    "                if it != 0 and self.error[1, -1] >= self.error[1, -1 - miniBatch]:\n",
    "                    stopCondition += 1\n",
    "                    if stopCondition == earlyStop:\n",
    "                        ind = self.error[2, :].argmin()\n",
    "                        return [wRecord[ind] for wRecord in self.wRecords]\n",
    "                else:\n",
    "                    stopCondition = 0\n",
    "            else:\n",
    "                print (trainLoss, trainError, testLoss, testError)\n",
    "            it += 1\n",
    "        if earlyStop:\n",
    "            ind = self.error[2, :].argmin()\n",
    "            weights = [wRecord[ind] for wRecord in self.wRecords]\n",
    "        return weights\n",
    "\n",
    "    def evalLossError(self, X, t, weights):\n",
    "        X = addBias(X)\n",
    "        outputs = self.forwardProp(X, weights)\n",
    "        y = outputs[-1]\n",
    "        n = t.size\n",
    "        loss = -np.sum(np.log(y[t, range(n)])) / n\n",
    "\n",
    "        for weight in weights:\n",
    "            loss += (np.sum(weight * weight)) * self.lam\n",
    "\n",
    "        prediction = y.argmax(axis = 0)\n",
    "        error = np.mean(prediction != t)\n",
    "        return loss, error\n",
    "\n",
    "    def forwardProp(self, X, weights):\n",
    "        outputs = [X]\n",
    "        for weight, actFun in zip(weights, self.actFuns):\n",
    "            if actFun == \"softMax\":\n",
    "                outputs.append(softMax(outputs[-1], weight))\n",
    "            elif actFun == \"ReLU\":\n",
    "                outputs.append(ReLU(outputs[-1], weight))\n",
    "            elif actFun == \"sigmoid\":\n",
    "                outputs.append(logistic(outputs[-1], weight))\n",
    "            elif actFun == \"tanh\":\n",
    "                outputs.append(tanh(outputs[-1], weight))\n",
    "        return outputs[1:]\n",
    "\n",
    "    def backProp(self, X, t, outputs, weights):\n",
    "        n = t.size\n",
    "        dW = []\n",
    "        outputs = [X] + outputs\n",
    "        for i in range(len(weights) - 1, -1, -1):\n",
    "            output1, output2 = outputs[i], outputs[i + 1]\n",
    "            if self.actFuns[i] == \"softMax\":\n",
    "                output2[t, range(n)] -= 1\n",
    "                delta = output2 / n\n",
    "                dW.append(np.dot(output1, delta.T))\n",
    "            elif self.actFuns[i] == \"ReLU\":\n",
    "                delta = (1. * (output2 > 0)) * np.dot(weights[i + 1], delta)\n",
    "                dW.append(np.dot(output1, delta.T))\n",
    "            elif self.actFuns[i] == \"sigmoid\":\n",
    "                output2[t, range(n)] -= 1\n",
    "                delta = output2 / n\n",
    "                dW.append(np.dot(output1, delta.T))\n",
    "            elif self.actFuns[i] == \"tanh\":\n",
    "                G = output2 / 1.7159\n",
    "                dG = (1 - np.power(G, 2)) * (2/3) * 1.7159\n",
    "                delta = dG * np.dot(weights[i+1], delta)\n",
    "                dW.append(np.dot(output1, delta.T))\n",
    "\n",
    "        dW = dW[::-1]\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            dW[i] += 2 * weights[i] * self.lam\n",
    "        return dW\n",
    "\n",
    "    def evalTest(self, xTest, tTest, weights = 0):\n",
    "        if type(weights) is int:\n",
    "            return self.evalLossError(xTest, tTest, self.W)\n",
    "        else:\n",
    "            return self.evalLossError(xTest, tTest, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (64,786) and (785,468) not aligned: 786 (dim 1) != 785 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-82cf5722cf13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmultilayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearlyStop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactFuns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tanh\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"softMax\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminiBatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnHiddenLayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-0777e32706df>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainData, trainLabel, nHiddenLayer, lr, anneal, maxIter, earlyStop, miniBatch, lam, momentum, actFuns)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnUnits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnUnits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnUnits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manneal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearlyStop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminiBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manneal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearlyStop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminiBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-0777e32706df>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, weights, lr, maxIter, anneal, earlyStop, miniBatch, mu)\u001b[0m\n\u001b[0;32m     45\u001b[0m                     \u001b[0mlr1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforwardProp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                 \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackProp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mtrainLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainError\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevalLossError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-0777e32706df>\u001b[0m in \u001b[0;36mforwardProp\u001b[1;34m(self, X, weights)\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mactFun\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"tanh\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                 \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a7af87cf3071>\u001b[0m in \u001b[0;36mtanh\u001b[1;34m(X, W)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1.7159\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (64,786) and (785,468) not aligned: 786 (dim 1) != 785 (dim 0)"
     ]
    }
   ],
   "source": [
    "multilayer = multiLayer(trainData, trainLabel, lr = 1e-1, earlyStop = False, maxIter = 100, actFuns=(\"tanh\", \"softMax\"), lam = 0., miniBatch = 128, nHiddenLayer = (64, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test_loss, test_error = multilayer.evalTest(testData, testLabel)\n",
    "print (1 - test_error)\n",
    "t1 = time.clock() - t0\n",
    "print ('Running Time =', t1)\n",
    "# multilayer.plot_losses()\n",
    "# multilayer.plot_errors()\n",
    "plotLoss(multilayer)\n",
    "plotError(multilayer)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 27",
   "language": "python",
   "name": "python27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
