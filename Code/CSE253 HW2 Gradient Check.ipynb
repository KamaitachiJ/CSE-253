{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((785L, 64L), (65L, 10L))\n",
      "2\n",
      "(65L, 10L)\n",
      "(64L, 1000L)\n",
      "(785L, 64L)\n",
      "9.84352625843e-06\n"
     ]
    }
   ],
   "source": [
    "from mnist import MNIST\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "mndata = MNIST('../mnist')\n",
    "trainData, trainLabel = mndata.load_training()\n",
    "testData, testLabel = mndata.load_testing()\n",
    "\n",
    "xTrain, yTrain = trainData[:1000], trainLabel[:1000]\n",
    "xTest, yTest = testData[-100:], testLabel[-100:]\n",
    "# unified data\n",
    "xTrain = np.array(xTrain).T / 255.\n",
    "yTrain = np.array(yTrain).T\n",
    "xTest = np.array(xTest).T / 255.\n",
    "yTest = np.array(yTest).T\n",
    "# zero mean and unit variance\n",
    "xTrain -= np.mean(xTrain, axis=0)\n",
    "xTrain /= np.std(xTrain, axis = 0)\n",
    "xTest -= np.mean(xTest, axis=0)\n",
    "xTest /= np.std(xTest, axis = 0)\n",
    "\n",
    "def addBias(data):\n",
    "    data = np.insert(data, data.shape[0], 1, axis=0)\n",
    "    return data\n",
    "# Activation Function\n",
    "def ReLU(X,W):\n",
    "    a = np.dot(W.T, X)\n",
    "    return a * (a > 0)\n",
    "\n",
    "def softMax(X, W):\n",
    "    ak = np.dot(W.T, X)\n",
    "    scores = np.exp(ak)\n",
    "    return scores / np.sum(scores, axis=0, keepdims=True)\n",
    "\n",
    "def tanh(X, W):\n",
    "    a = np.dot(W.T, X)\n",
    "    return 1.7159 * np.tanh((2/3) * a)\n",
    "# Forward propagation\n",
    "def forwardProp(X, weights, actFncs):\n",
    "    X = addBias(X)\n",
    "    inputs = [X]\n",
    "    outputs = []\n",
    "    for weight, actFun in zip(weights, actFncs):\n",
    "        if actFun == \"softMax\":\n",
    "            outputs.append(softMax(inputs[-1], weight))\n",
    "            inputs.append(addBias(outputs[-1]))\n",
    "        elif actFun == \"ReLU\":\n",
    "            outputs.append(softMax(inputs[-1], weight))\n",
    "            inputs.append(addBias(outputs[-1]))\n",
    "        elif actFun == \"tanh\":\n",
    "            outputs.append(softMax(inputs[-1], weight))\n",
    "            inputs.append(addBias(outputs[-1]))\n",
    "    return outputs\n",
    "# Back prapagation\n",
    "def backProp(X, t, outputs, weights, actFncs, lam):\n",
    "    X = addBias(X)\n",
    "    n = t.size\n",
    "    dW = []\n",
    "    outputs = [X] + outputs\n",
    "    for i in range(len(weights) - 1, -1, -1):\n",
    "        output1, output2 = outputs[i], outputs[i + 1]\n",
    "        if actFncs[i] == \"softMax\":\n",
    "            output2[t, range(n)] -= 1\n",
    "            delta = output2 / n\n",
    "            dW.append(np.dot(addBias(output1), delta.T))\n",
    "            print(np.dot(addBias(output1), delta.T).shape)\n",
    "        elif actFncs[i] == \"ReLU\":\n",
    "            delta = (1. * (output2 > 0)) * np.dot(weights[i + 1], delta)\n",
    "            dW.append(np.dot(output1, delta.T))\n",
    "        elif actFncs[i] == \"tanh\":\n",
    "            G = output2 / 1.7159\n",
    "            dG = (1 - np.power(G, 2)) * (2/3) * 1.7159\n",
    "            delta = dG * np.dot(weights[i+1], delta)[1:]\n",
    "            print(delta.shape)\n",
    "            dW.append(np.dot(output1, delta.T))\n",
    "            print(np.dot(output1, delta.T).shape)\n",
    "    dW = dW[::-1]\n",
    "    for i in range(len(weights)):\n",
    "        dW[i] += 2 * weights[i] * lam\n",
    "    return dW\n",
    "# Cross-entropy\n",
    "def crossEntropy(X, t, weights, actFncs, lam):\n",
    "    predY = forwardProp(X, weights, actFncs)\n",
    "    Y = predY[-1]\n",
    "    n = t.size\n",
    "    loss = -np.sum(np.log(Y[t, range(n)])) / n\n",
    "    for weight in weights:\n",
    "        loss += lam * np.sum(np.square(weight))\n",
    "    return loss\n",
    "# Numerical gradient\n",
    "def numericGrad(X, t, weights, epsilon, actFncs, lam):\n",
    "    Grad = []\n",
    "    for l in range(len(weights)):\n",
    "        xx, yy = weights[l].shape\n",
    "        numGrad = np.zeros((xx, yy))\n",
    "        xAxis, yAxis = np.meshgrid(range(xx), range(yy))\n",
    "        for i,j in zip(xAxis, yAxis):\n",
    "            adjweight_1 = adjweight_2 = weights\n",
    "            adjweight_1[l][i, j] += epsilon\n",
    "            adjweight_2[l][i, j] -= epsilon\n",
    "            entropy1 = crossEntropy(X, t, adjweight_1, actFncs, lam)\n",
    "            entropy2 = crossEntropy(X, t, adjweight_2, actFncs, lam)\n",
    "            numGrad[i ,j] = (entropy1 - entropy2) / (2 * epsilon)\n",
    "        Grad.append(numGrad)\n",
    "    return Grad\n",
    "# Check gradient difference\n",
    "def checkGradient(X, t, actFncs, layers, epsilon, lam = 1e-3):\n",
    "    nLayer = len(layers) + 1\n",
    "    nClass = len(np.unique(t))\n",
    "    nHiddenLayers = ()\n",
    "    # for i in range(len(layers)):\n",
    "    #     nHiddenLayers += (layers[i] + 1, )\n",
    "    nFanin = (X.shape[0], ) + layers + (nClass, )\n",
    "    wInit = [1 / np.sqrt(nFanin[i]) * np.random.randn(nFanin[i]+1, nFanin[i+1]) for i in range(nLayer)]\n",
    "    print(wInit[0].shape, wInit[1].shape)\n",
    "    hiddenOut = forwardProp(X, wInit, actFncs)\n",
    "    print(len(hiddenOut))\n",
    "    trueGradient = backProp(X, t, hiddenOut, wInit, actFncs, lam)\n",
    "    numGrandient = numericGrad(X, t, wInit, epsilon, actFncs, lam)\n",
    "    diff = 0\n",
    "    for k in range(len(trueGradient)):\n",
    "        diff += np.abs(np.mean(trueGradient[k] - numGrandient[k]))\n",
    "    return diff\n",
    "\n",
    "d = checkGradient(xTrain, yTrain, ('tanh', 'softMax'), (64, ), 1e-2)\n",
    "print d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 27",
   "language": "python",
   "name": "python27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
