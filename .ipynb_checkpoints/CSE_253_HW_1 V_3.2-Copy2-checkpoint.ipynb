{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Read in the data from the files.\n",
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "mndata = MNIST('../../HW1/mnist')\n",
    "\n",
    "trainData, trainLabel = mndata.load_training()\n",
    "\n",
    "testData, testLabel = mndata.load_testing()\n",
    "\n",
    "trainData = np.asarray(trainData)\n",
    "testData = np.asarray(testData)\n",
    "trainLabel = np.asarray(trainLabel)[:, np.newaxis]\n",
    "testLabel = np.asarray(testLabel)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2adedd57e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Show a templete of digits.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "img = np.reshape(trainData[0],(28,28))\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap=cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def addBias(data):\n",
    "    bias = np.ones(len(data))[:, np.newaxis]\n",
    "    biasData = np.concatenate((bias,data), axis=1)\n",
    "    return biasData\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def oneHot(label):\n",
    "    labelHot = np.zeros((len(label),len(np.unique(label))))\n",
    "    for i in range(len(label)):\n",
    "        labelHot[i,label[i]] = 1\n",
    "    return labelHot\n",
    "\n",
    "def calculateLoss(data, label, w1, w2, lam):\n",
    "    a_j = data.dot(w1)\n",
    "    ga_j = sigmoid(a_j)\n",
    "    a_k = addBias(ga_j).dot(w2)\n",
    "    ga_k = np.exp(a_k) / np.sum(np.exp(a_k), axis=1)[:, np.newaxis]\n",
    "    E = -1.0 * np.mean(label * np.log(ga_k))\n",
    "    C = 0.5 * (np.sum(np.power(w1, 2)) + np.sum(np.power(w2, 2)))\n",
    "    loss = E + lam * C\n",
    "    #loss = E\n",
    "    return loss\n",
    "\n",
    "def pecisionRate(data, label, w1, w2):\n",
    "    data = data / 127.5 - 1\n",
    "    data = addBias(data)\n",
    "    label = oneHot(label)\n",
    "    correct = 0\n",
    "    for i in range(len(data)):\n",
    "        a_j = data[i][np.newaxis,:].dot(w1)\n",
    "        ga_j = sigmoid(a_j)\n",
    "        a_k = addBias(ga_j).dot(w2)\n",
    "        ga_k = (np.exp(a_k) / np.sum(np.exp(a_k), axis=1)[:, np.newaxis])[0]\n",
    "        prediction = np.where(ga_k == np.max(ga_k))[0][0]\n",
    "        if label[i, prediction] == 1:\n",
    "            correct += 1\n",
    "    rate = 1.0 * correct / len(label)\n",
    "    return rate\n",
    "\n",
    "def oneHiddenLayer(trainData, trainLabel, lr = 1e-1, maxIter = 500, T = 2e8, lam = 1e-3, batchSize = 1, unitNum = 64):\n",
    "    # Process the range of pixel value to [-1..1]\n",
    "    trainData = trainData / 127.5 - 1\n",
    "\n",
    "    # tack on a \"1\" at the beginning for a bias parameter\n",
    "    trainData = addBias(trainData)\n",
    "    \n",
    "    classNum = len(np.unique(trainLabel))\n",
    "    trainLabel = oneHot(trainLabel)\n",
    "    \n",
    "    trainD, validD, trainL, validL = train_test_split(trainData, trainLabel, test_size=0.1, random_state=42)    \n",
    "\n",
    "    w_ij = np.random.normal(0,1,(len(trainD[0]), unitNum))\n",
    "    w_jk = np.random.normal(0,1,(unitNum + 1, classNum))\n",
    "    \n",
    "    index = np.arange(len(trainD))\n",
    "    it = 0\n",
    "    \n",
    "    while it <= maxIter:\n",
    "        np.random.shuffle(index)\n",
    "        lr1 = lr / (1.0 + it / T)\n",
    "        # Mini Batch Gradient Descent\n",
    "        for startInd in range(0, len(trainData) - batchSize + 1, batchSize):\n",
    "            trainDBatch = trainD[index[startInd : startInd + batchSize]]\n",
    "            #trainDMean = np.mean(trainDBatch, axis = 0)[np.newaxis, :]\n",
    "            #trainDBatch = trainDBatch - trainDMean\n",
    "            trainLBatch = trainL[index[startInd : startInd + batchSize]]\n",
    "            \n",
    "            #Forward Propagation\n",
    "            #1. Input to Hidden Layer\n",
    "            a_j = trainDBatch.dot(w_ij)   #(batchSize, unitNum) \n",
    "            ga_j = addBias(sigmoid(a_j))   #(batchSize, unitNum + 1)\n",
    "            \n",
    "            #2. Hidden Layer to Output\n",
    "            a_k = ga_j.dot(w_jk)   #(batchSize, classNum)\n",
    "            ga_k = np.exp(a_k) / np.sum(np.exp(a_k), axis=1)[:, np.newaxis]   #(batchSize, classNum)\n",
    "            \n",
    "            #Back Propagation\n",
    "            delta_k = ga_k - trainLBatch   #(batchSize, classNum)\n",
    "            derGa_j = ga_j * (1 - ga_j)   #(batchSize, unitNum)\n",
    "            delta_j = (derGa_j * (delta_k.dot(w_jk.T)))[:,1:]   #(batchSize, unitNum)\n",
    "            \n",
    "            # Update weights\n",
    "            w_jk = w_jk - lr1 * (ga_j.T.dot(delta_k) / batchSize + lam * np.sum(w_jk))  #(unitNum + 1, classNum)\n",
    "            w_ij = w_ij - lr1 * (trainDBatch.T.dot(delta_j) / batchSize + lam * np.sum(w_ij))   #(featuresNum, unitNum)\n",
    "\n",
    "        lossV = calculateLoss(validD, validL, w_ij, w_jk, lam)\n",
    "        if it % 10 == 0:\n",
    "            print (lossV)\n",
    "        it += 1\n",
    "    print ('done')\n",
    "    return w_ij, w_jk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0766942498167\n",
      "0.0523781013265\n",
      "0.0465750318255\n",
      "0.0477010975476\n",
      "0.0472935931016\n",
      "0.0472683471473\n",
      "0.0486768238388\n",
      "0.0495069807376\n",
      "0.050380521659\n",
      "0.0515151873797\n",
      "0.050986894879\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "w_ij, w_jk = oneHiddenLayer(trainData, trainLabel, lr = 1e-0, maxIter = 100, lam = 1e-6, batchSize = int(len(trainData) / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9481"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = pecisionRate(testData, testLabel, w_ij, w_jk)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4448.79949585, -4761.01029841, -6902.33407442,  3573.96886495,\n",
       "        -6593.45035964, -5871.78965164, -7201.56107868,  1593.02459103,\n",
       "        -4328.62073898, -2791.58341406,  2651.95329336,  6675.08036278,\n",
       "        -4826.43375407,  5641.79183409, -5984.56785664,  5385.87959803,\n",
       "        -4666.34612753,   374.37642309, -1115.83681333,  -608.65957331,\n",
       "         -926.40658808, -3617.92612437,  4338.3210159 ,  1506.36191353,\n",
       "        -2491.59229304, -1340.87929797,   167.33066134, -2171.64609388,\n",
       "        -8980.71290199,   960.61408883,   331.36578918,  6449.2575138 ,\n",
       "        -1421.68296837,   124.25198384,  1043.2963955 , -6429.88404267,\n",
       "        -3934.11478998,   183.13028233,  6592.63831995,  4768.62138868,\n",
       "         1904.4483029 ,  2348.10309259,  -107.1413059 ,  1272.60097793,\n",
       "        -4816.76907502,  3733.6018195 ,   406.39979207,  3738.14500142,\n",
       "          536.83236285,  -100.68815461,   709.65708274,  3452.69379949,\n",
       "        -2535.16865074,  1386.6007308 ,  3630.1532556 , -3498.11991656,\n",
       "          939.93077465, -1052.13818816, -2141.57087721, -4570.8361945 ,\n",
       "        -6909.60546372,  -967.68087162, -3100.53772007, -1169.37263696]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTest = addBias(testData)\n",
    "a_j = xTest[435][np.newaxis,:].dot(w_ij)\n",
    "a_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
       "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
       "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000,\n",
       "          0.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
       "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000,\n",
       "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
       "          0.00000000e+000,   4.59733292e-265,   0.00000000e+000,\n",
       "          0.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
       "          0.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
       "          0.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
       "          1.00000000e+000,   1.00000000e+000,   0.00000000e+000,\n",
       "          1.00000000e+000,   1.00000000e+000,   0.00000000e+000,\n",
       "          0.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
       "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
       "          2.94524931e-047,   1.00000000e+000,   0.00000000e+000,\n",
       "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
       "          1.00000000e+000,   1.86934759e-044,   1.00000000e+000,\n",
       "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
       "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
       "          0.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
       "          0.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
       "          0.00000000e+000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga_j = sigmoid(a_j)\n",
    "ga_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.82517774,  -4.64466323,   1.78655806,   3.35316663,\n",
       "        -16.32437168,  -6.4948138 ,  -1.41853248,  -8.79218931,\n",
       "         28.48344407,  -1.36687991]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_k = addBias(ga_j).dot(w_jk)\n",
    "a_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.73122582e-13,   4.09869689e-15,   2.54501134e-12,\n",
       "         1.21915571e-11,   3.46906847e-20,   6.44370455e-16,\n",
       "         1.03213519e-13,   6.47736117e-17,   1.00000000e+00,\n",
       "         1.08684850e-13])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga_k = (np.exp(a_k) / np.sum(np.exp(a_k), axis=1)[:, np.newaxis])[0]\n",
    "ga_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.where(ga_k == np.max(ga_k))[0][0]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLabel[435]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwg8kRfiBpAg/kBThB5L6fy6u8I7yiKuKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2adf43087b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = np.reshape(xTest[435, 1:],(28,28))\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap=cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batchSize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-309-a8784d30bb26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mdelta_j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mderGa_j\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdelta_k\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_jk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;31m#(batchSize, unitNum)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mw_ij\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_ij\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mxTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_j\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatchSize\u001b[0m   \u001b[1;31m#(featuresNum, unitNum)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mw_jk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_jk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mga_j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_k\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatchSize\u001b[0m   \u001b[1;31m#(unitNum, classNum)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batchSize' is not defined"
     ]
    }
   ],
   "source": [
    "a_j = xTrain[:5].dot(w_ij)   #(batchSize, unitNum) \n",
    "\n",
    "ga_j = sigmoid(a_j)   #(batchSize, unitNum)\n",
    "\n",
    "a_k = addBias(ga_j).dot(w_jk)   #(batchSize, classNum)\n",
    "\n",
    "ga_k = np.exp(a_k) / np.sum(np.exp(a_k), axis=1)[:, np.newaxis]   #(batchSize, classNum)\n",
    "\n",
    "derGa_j = ga_j * (1 - ga_j)   #(batchSize, unitNum)\n",
    "\n",
    "delta_k = yTrHot[:5] - ga_k   #(batchSize, classNum)\n",
    "\n",
    "delta_j = derGa_j * (delta_k.dot(w_jk.T))[:,1:]   #(batchSize, unitNum)\n",
    "\n",
    "w_ij = w_ij + 1e-3 * xTrain[:5].T.dot(delta_j) / batchSize   #(featuresNum, unitNum)\n",
    "\n",
    "w_jk = w_jk + 1e-3 * ga_j.T.dot(delta_k) / batchSize   #(unitNum, classNum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_images():\n",
    "    # load the MNIST data set\n",
    "    mndata = MNIST('../../HW1/mnist')\n",
    "    mndata.load_training()\n",
    "    mndata.load_testing()\n",
    "\n",
    "    # using numpy array form to store the train and test sets\n",
    "    # and transform the data type to double\n",
    "    train_images = np.array(mndata.train_images).T / 255.\n",
    "    train_labels = np.array(mndata.train_labels).T\n",
    "    test_images = np.array(mndata.test_images).T / 255.\n",
    "    test_labels = np.array(mndata.test_labels).T\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "# pre-process the data so that each pixel will have roughly zero mean and unit variance.\n",
    "def whitening(images):\n",
    "    m = np.mean(images, axis=0)\n",
    "    images -= m\n",
    "    return images\n",
    "\n",
    "\n",
    "# insert an extra dimension for the bias term\n",
    "def insert_bias(X):\n",
    "    X = np.insert(X, X.shape[0], 1, axis=0)\n",
    "    return X\n",
    "\n",
    "\n",
    "# randomly shuffle the data\n",
    "def shuffle(X, t):\n",
    "    idx = np.random.permutation(t.size)\n",
    "    X, t = X[:, idx], t[idx]\n",
    "    return X, t\n",
    "\n",
    "\n",
    "def plot_losses(method):\n",
    "    fig, ax = plt.subplots()\n",
    "    niter = method.losses.shape[1]\n",
    "    x = np.linspace(1, niter / method.mini_batch, niter)\n",
    "    ax.plot(x, method.losses[0], label=\"train loss\")\n",
    "    if method.early_stop:\n",
    "        ax.plot(x, method.losses[1], label=\"validation loss\")\n",
    "        ax.plot(x, method.losses[2], label=\"test loss\")\n",
    "    else:\n",
    "        ax.plot(x, method.losses[1], label=\"test loss\")\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def plot_errors(method):\n",
    "    fig, ax = plt.subplots()\n",
    "    niter = method.errors.shape[1]\n",
    "    x = np.linspace(1, niter / method.mini_batch, niter)\n",
    "    ax.plot(x, 1 - method.errors[0], label=\"train percent correct\")\n",
    "    if method.early_stop:\n",
    "        ax.plot(x, 1 - method.errors[1], label=\"validation percent correct\")\n",
    "        ax.plot(x, 1 - method.errors[2], label=\"test percent correct\")\n",
    "    else:\n",
    "        ax.plot(x, 1 - method.errors[1], label=\"test percent correct\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "def sigmoid(X, W):\n",
    "    return 1 / (1 + np.exp(np.dot(W.T, X)))\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def dReLU(x):\n",
    "    return 1. * (x > 0)\n",
    "\n",
    "def softmax(X, W):\n",
    "    ak = np.dot(W.T, X)\n",
    "    scores = np.exp(ak)\n",
    "    return scores / np.sum(scores, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def tanh_sigmoid(X, W):\n",
    "    a = np.dot(W.T, X)\n",
    "    return 1.7159 * np.tanh(2.0 / 3.0 * a)\n",
    "\n",
    "\n",
    "class MultilayerClassifier():\n",
    "    def __init__(self, train_images, train_labels, nClasses=10, nHiddenUnits=(100,), shuffle=True,\n",
    "                 learning_rate=1., anneal=0., early_stop=3, mini_batch=1, reg_type=\"\", reg_weight=0., method=\"GD\",\n",
    "                 momentum=0.9, act_funs=(\"sigmoid\", \"softmax\"), custom_init_weight=0.):\n",
    "        self.X, self.t = train_images, train_labels\n",
    "        self.X = whitening(self.X)\n",
    "        self.X = insert_bias(self.X)\n",
    "        self.early_stop = early_stop\n",
    "        if early_stop:\n",
    "            validation_size = int(self.X.shape[1] / 6)\n",
    "            self.X_validation, self.t_validation = self.X[:, :validation_size], self.t[:validation_size]\n",
    "            self.X, self.t = self.X[:, validation_size:], self.t[validation_size:]\n",
    "        self.act_funs = act_funs\n",
    "        self.reg_type = reg_type\n",
    "        self.reg_weight = reg_weight\n",
    "        self.mini_batch = mini_batch\n",
    "        self.shuffle = shuffle\n",
    "        self.losses = np.array([[], [], []]) if early_stop else np.array([[], []])\n",
    "        self.errors = np.array([[], [], []]) if early_stop else np.array([[], []])\n",
    "        nLayer = len(nHiddenUnits) + 1\n",
    "        nUnits = (self.X.shape[0],) + nHiddenUnits + (nClasses,)\n",
    "        if not custom_init_weight:\n",
    "            self.weights = [1 / np.sqrt(nUnits[i]) * np.random.randn(nUnits[i], nUnits[i + 1]) for i in range(nLayer)]\n",
    "        else:\n",
    "            self.weights = [custom_init_weight * np.random.randn(nUnits[i], nUnits[i + 1]) for i in range(nLayer)]\n",
    "        if method:\n",
    "            self.weights = self.optimization(self.weights, learning_rate, anneal, early_stop, mini_batch, method, momentum)\n",
    "\n",
    "    def optimization(self, weights, learning_rate, anneal, early_stop, mini_batch, method, mu):\n",
    "        d, n = self.X.shape\n",
    "        batch_size = n / mini_batch\n",
    "        weights_records = [[] for _ in weights]\n",
    "        velocities = [0 for _ in weights]\n",
    "        train_loss = train_error = validation_loss = validation_error = test_loss = test_error = 0\n",
    "        for i in range(2000):\n",
    "            start, end = 0, batch_size\n",
    "            if self.shuffle:\n",
    "                self.X, self.t = shuffle(self.X, self.t)\n",
    "            for j in range(mini_batch):\n",
    "                X_batch, t_batch = self.X[:, start:end], self.t[start:end]\n",
    "\n",
    "                lr = learning_rate if not anneal else \\\n",
    "                    learning_rate / (1. + i / anneal) if i * mini_batch + j > 5 else 1e0\n",
    "\n",
    "                outputs = self.forwardProp(X_batch, weights)\n",
    "                dWeights = self.backProp(X_batch, t_batch, outputs, weights)\n",
    "                train_loss, train_error = self.eval_loss_and_error(X_batch, t_batch, weights)\n",
    "\n",
    "                if j % 100 == 0:\n",
    "                    test_loss, test_error = self.test(test_images, test_labels, weights)\n",
    "\n",
    "                for k in range(len(weights_records)):\n",
    "                    weights_records[k].append(np.array(weights[k]))\n",
    "\n",
    "                if early_stop:\n",
    "                    if j % 100 == 0:\n",
    "                        validation_loss, validation_error = self.eval_loss_and_error(self.X_validation, self.t_validation, weights)\n",
    "\n",
    "                    self.losses = np.hstack([self.losses, [[train_loss], [validation_loss], [test_loss]]])\n",
    "                    self.errors = np.hstack([self.errors, [[train_error], [validation_error], [test_error]]])\n",
    "                    #print train_loss, train_error, validation_loss, validation_error, test_loss, test_error\n",
    "                else:\n",
    "                    self.losses = np.hstack([self.losses, [[train_loss], [test_loss]]])\n",
    "                    self.errors = np.hstack([self.errors, [[train_error], [test_error]]])\n",
    "                    #print train_loss, train_error, test_loss, test_error\n",
    "\n",
    "                if method == \"GD\":\n",
    "                    weights = map(lambda x, y: x - lr * y, weights, dWeights)\n",
    "                elif method == \"NAG\":\n",
    "                    for k in range(len(weights)):\n",
    "                        v_prev = velocities[k]\n",
    "                        velocities[k] = mu * velocities[k] - lr * dWeights[k]\n",
    "                        weights[k] += -mu * v_prev + (1 + mu) * velocities[k]\n",
    "\n",
    "                start, end = start + batch_size, end + batch_size if j != mini_batch - 1 else n\n",
    "\n",
    "            #test_loss, test_error = self.test(test_images, test_labels, weights)\n",
    "            if early_stop:\n",
    "                print (train_loss, train_error, validation_loss, validation_error, test_loss, test_error)\n",
    "                if i != 0 and self.errors[1, -1] >= self.errors[1, -1 - mini_batch]:\n",
    "                    up_epoch += 1\n",
    "                    if up_epoch == early_stop:\n",
    "                        idx = self.errors[1, :].argmin()\n",
    "                        return [weights_record[idx] for weights_record in weights_records]\n",
    "                else:\n",
    "                    up_epoch = 0\n",
    "            else:\n",
    "                print (train_loss, train_error, test_loss, test_error)\n",
    "\n",
    "        if early_stop:\n",
    "            idx = self.errors[1, :].argmin()\n",
    "            weights = [weights_record[idx] for weights_record in weights_records]\n",
    "        return weights\n",
    "\n",
    "    def eval_loss_and_error(self, X, t, weights):\n",
    "        outputs = self.forwardProp(X, weights)\n",
    "        y = outputs[-1]\n",
    "        n = t.size\n",
    "        loss = -np.sum(np.log(y[t, range(n)])) / n\n",
    "\n",
    "        for weight in weights:\n",
    "            if self.reg_type == \"L2\":\n",
    "                loss += (np.sum(weight * weight)) * self.reg_weight\n",
    "            elif self.reg_type == \"L1\":\n",
    "                loss += (np.sum(np.abs(weight))) * self.reg_weight\n",
    "\n",
    "        predict = y.argmax(axis=0)\n",
    "        error = np.mean(predict != t)\n",
    "        return loss, error\n",
    "\n",
    "    def forwardProp(self, X, weights):\n",
    "        outputs = [X]\n",
    "        for weight, act_fun in zip(weights, self.act_funs):\n",
    "            if act_fun == \"sigmoid\":\n",
    "                outputs.append(sigmoid(outputs[-1], weight))\n",
    "            elif act_fun == \"softmax\":\n",
    "                outputs.append(softmax(outputs[-1], weight))\n",
    "            elif act_fun == \"tanh\":\n",
    "                outputs.append(tanh_sigmoid(outputs[-1], weight))\n",
    "        return outputs[1:]\n",
    "\n",
    "    def backProp(self, X, t, outputs, weights):\n",
    "        n = t.size\n",
    "        dWeights = []\n",
    "        outputs = [X] + outputs\n",
    "        for i in range(len(weights) - 1, -1, -1):\n",
    "            output1, output2 = outputs[i], outputs[i + 1]\n",
    "            if self.act_funs[i] == \"softmax\":\n",
    "                output2[t, range(n)] -= 1  # computing y - t\n",
    "                delta = output2 / n\n",
    "                dWeights.append(np.dot(output1, delta.T))\n",
    "            elif self.act_funs[i] == \"sigmoid\":\n",
    "                delta = -output2 * (1 - output2) * np.dot(weights[i + 1], delta)\n",
    "                dWeights.append(np.dot(output1, delta.T))\n",
    "            elif self.act_funs[i] == \"tanh\":\n",
    "                f = output2 / 1.7159\n",
    "                delta = 1.14393 * (1 - f ** 2) * np.dot(weights[i + 1], delta)\n",
    "                dWeights.append(np.dot(output1, delta.T))\n",
    "        dWeights = dWeights[::-1]\n",
    "\n",
    "        for i in range(len(weights)):\n",
    "            if self.reg_type == \"L2\":\n",
    "                dWeights[i] += 2 * weights[i] * self.reg_weight\n",
    "            elif self.reg_type == \"L1\":\n",
    "                regW = np.array(weights[i])\n",
    "                regW[regW>0] = 1\n",
    "                regW[regW<0] = -1\n",
    "                dWeights[i] += self.reg_weight * regW\n",
    "\n",
    "        return dWeights\n",
    "\n",
    "    # test on the test set, if weight not defined then use self.weight\n",
    "    def test(self, X_test, t_test, weights=0):\n",
    "        #X_test = insert_bias(whitening(X_test))\n",
    "\n",
    "        if type(weights) is int:\n",
    "            return self.eval_loss_and_error(X_test, t_test, self.weights)\n",
    "        else:\n",
    "            return self.eval_loss_and_error(X_test, t_test, weights)\n",
    "\n",
    "    def plot_losses(self):\n",
    "        plot_losses(self)\n",
    "\n",
    "    def plot_errors(self):\n",
    "        plot_errors(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images, train_labels, test_images, test_labels = load_images()\n",
    "test_images = insert_bias(whitening(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-11fa4ff5e399>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m multilayer = MultilayerClassifier(train_images, train_labels, learning_rate=1e-1, early_stop=3, method=\"NAG\",\n\u001b[0;32m      2\u001b[0m                                       \u001b[0mact_funs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'L2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                       nHiddenUnits=(50, 50))\n\u001b[0m",
      "\u001b[1;32m<ipython-input-71-664881215f47>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_images, train_labels, nClasses, nHiddenUnits, shuffle, learning_rate, anneal, early_stop, mini_batch, reg_type, reg_weight, method, momentum, act_funs, custom_init_weight)\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcustom_init_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnUnits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnUnits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manneal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moptimization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manneal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-664881215f47>\u001b[0m in \u001b[0;36moptimization\u001b[1;34m(self, weights, learning_rate, anneal, early_stop, mini_batch, method, mu)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                 \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0manneal\u001b[0m \u001b[1;32melse\u001b[0m                     \u001b[0mlearning_rate\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0manneal\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1e0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "multilayer = MultilayerClassifier(train_images, train_labels, learning_rate=1e-1, early_stop=3, method=\"NAG\",\n",
    "                                      act_funs=('tanh', 'tanh','softmax'), reg_type='L2', reg_weight=0, mini_batch=400,\n",
    "                                      nHiddenUnits=(50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
