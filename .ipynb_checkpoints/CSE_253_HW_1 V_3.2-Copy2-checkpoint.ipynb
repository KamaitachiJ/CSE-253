{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Read in the data from the files.\n",
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "mndata = MNIST('../../HW1/mnist')\n",
    "\n",
    "trainData, trainLabel = mndata.load_training()\n",
    "\n",
    "testData, testLabel = mndata.load_testing()\n",
    "\n",
    "trainData = np.asarray(trainData)\n",
    "testData = np.asarray(testData)\n",
    "trainLabel = np.asarray(trainLabel)[:, np.newaxis]\n",
    "testLabel = np.asarray(testLabel)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADgpJREFUeJzt3X+MVfWZx/HPs1j+kKI4aQRCYSnE\nYJW4082IjSWrxkzVDQZHrekkJjQapn8wiU02ZA3/VNNgyCrslmiamaZYSFpKE3VB0iw0otLGZuKI\nWC0srTFsO3IDNTjywx9kmGf/mEMzxbnfe+fec++5zPN+JeT+eM6558kNnznn3O+592vuLgDx/EPR\nDQAoBuEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxDUZc3cmJlxOSHQYO5u1SxX157fzO40syNm\n9q6ZPVrPawFoLqv12n4zmybpj5I6JQ1Jel1St7sfSqzDnh9osGbs+ZdJetfd33P3c5J+IWllHa8H\noInqCf88SX8Z93goe+7vmFmPmQ2a2WAd2wKQs3o+8Jvo0OJzh/Xu3i+pX+KwH2gl9ez5hyTNH/f4\ny5KO1dcOgGapJ/yvS7rGzL5iZtMlfVvSrnzaAtBoNR/2u/uImfVK2iNpmqQt7v6H3DoD0FA1D/XV\ntDHO+YGGa8pFPgAuXYQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF\n+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8E\nVfMU3ZJkZkclnZZ0XtKIu3fk0RTyM23atGT9yiuvbOj2e3t7y9Yuv/zy5LpLlixJ1tesWZOsP/XU\nU2Vr3d3dyXU//fTTZH3Dhg3J+uOPP56st4K6wp+5zd0/yOF1ADQRh/1AUPWG3yXtNbM3zKwnj4YA\nNEe9h/3fcPdjZna1pF+b2f+6+/7xC2R/FPjDALSYuvb87n4suz0h6QVJyyZYpt/dO/gwEGgtNYff\nzGaY2cwL9yV9U9I7eTUGoLHqOeyfLekFM7vwOj939//JpSsADVdz+N39PUn/lGMvU9aCBQuS9enT\npyfrN998c7K+fPnysrVZs2Yl173vvvuS9SINDQ0l65s3b07Wu7q6ytZOnz6dXPett95K1l999dVk\n/VLAUB8QFOEHgiL8QFCEHwiK8ANBEX4gKHP35m3MrHkba6L29vZkfd++fcl6o79W26pGR0eT9Yce\neihZP3PmTM3bLpVKyfqHH36YrB85cqTmbTeau1s1y7HnB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGg\nGOfPQVtbW7I+MDCQrC9atCjPdnJVqffh4eFk/bbbbitbO3fuXHLdqNc/1ItxfgBJhB8IivADQRF+\nICjCDwRF+IGgCD8QVB6z9IZ38uTJZH3t2rXJ+ooVK5L1N998M1mv9BPWKQcPHkzWOzs7k/WzZ88m\n69dff33Z2iOPPJJcF43Fnh8IivADQRF+ICjCDwRF+IGgCD8QFOEHgqr4fX4z2yJphaQT7r40e65N\n0g5JCyUdlfSAu6d/6FxT9/v89briiiuS9UrTSff19ZWtPfzww8l1H3zwwWR9+/btyTpaT57f5/+p\npDsveu5RSS+5+zWSXsoeA7iEVAy/u++XdPElbCslbc3ub5V0T859AWiwWs/5Z7t7SZKy26vzawlA\nMzT82n4z65HU0+jtAJicWvf8x81sriRltyfKLeju/e7e4e4dNW4LQAPUGv5dklZl91dJ2plPOwCa\npWL4zWy7pN9JWmJmQ2b2sKQNkjrN7E+SOrPHAC4hFc/53b27TOn2nHsJ69SpU3Wt/9FHH9W87urV\nq5P1HTt2JOujo6M1bxvF4go/ICjCDwRF+IGgCD8QFOEHgiL8QFBM0T0FzJgxo2ztxRdfTK57yy23\nJOt33XVXsr53795kHc3HFN0Akgg/EBThB4Ii/EBQhB8IivADQRF+ICjG+ae4xYsXJ+sHDhxI1oeH\nh5P1l19+OVkfHBwsW3vmmWeS6zbz/+ZUwjg/gCTCDwRF+IGgCD8QFOEHgiL8QFCEHwiKcf7gurq6\nkvVnn302WZ85c2bN2163bl2yvm3btmS9VCrVvO2pjHF+AEmEHwiK8ANBEX4gKMIPBEX4gaAIPxBU\nxXF+M9siaYWkE+6+NHvuMUmrJf01W2ydu/+q4sYY57/kLF26NFnftGlTsn777bXP5N7X15esr1+/\nPll///33a972pSzPcf6fSrpzguf/093bs38Vgw+gtVQMv7vvl3SyCb0AaKJ6zvl7zez3ZrbFzK7K\nrSMATVFr+H8kabGkdkklSRvLLWhmPWY2aGblf8wNQNPVFH53P+7u5919VNKPJS1LLNvv7h3u3lFr\nkwDyV1P4zWzuuIddkt7Jpx0AzXJZpQXMbLukWyV9ycyGJH1f0q1m1i7JJR2V9N0G9gigAfg+P+oy\na9asZP3uu+8uW6v0WwFm6eHqffv2JeudnZ3J+lTF9/kBJBF+ICjCDwRF+IGgCD8QFOEHgmKoD4X5\n7LPPkvXLLktfhjIyMpKs33HHHWVrr7zySnLdSxlDfQCSCD8QFOEHgiL8QFCEHwiK8ANBEX4gqIrf\n50dsN9xwQ7J+//33J+s33nhj2VqlcfxKDh06lKzv37+/rtef6tjzA0ERfiAowg8ERfiBoAg/EBTh\nB4Ii/EBQjPNPcUuWLEnWe3t7k/V77703WZ8zZ86ke6rW+fPnk/VSqZSsj46O5tnOlMOeHwiK8ANB\nEX4gKMIPBEX4gaAIPxAU4QeCqjjOb2bzJW2TNEfSqKR+d/+hmbVJ2iFpoaSjkh5w9w8b12pclcbS\nu7u7y9YqjeMvXLiwlpZyMTg4mKyvX78+Wd+1a1ee7YRTzZ5/RNK/uftXJX1d0hozu07So5Jecvdr\nJL2UPQZwiagYfncvufuB7P5pSYclzZO0UtLWbLGtku5pVJMA8jepc34zWyjpa5IGJM1295I09gdC\n0tV5Nwegcaq+tt/MvijpOUnfc/dTZlVNByYz65HUU1t7ABqlqj2/mX1BY8H/mbs/nz193MzmZvW5\nkk5MtK6797t7h7t35NEwgHxUDL+N7eJ/Iumwu28aV9olaVV2f5Wknfm3B6BRKk7RbWbLJf1G0tsa\nG+qTpHUaO+//paQFkv4s6VvufrLCa4Wconv27NnJ+nXXXZesP/3008n6tddeO+me8jIwMJCsP/nk\nk2VrO3em9xd8Jbc21U7RXfGc391/K6nci90+maYAtA6u8AOCIvxAUIQfCIrwA0ERfiAowg8ExU93\nV6mtra1sra+vL7lue3t7sr5o0aKaesrDa6+9lqxv3LgxWd+zZ0+y/sknn0y6JzQHe34gKMIPBEX4\ngaAIPxAU4QeCIvxAUIQfCCrMOP9NN92UrK9duzZZX7ZsWdnavHnzauopLx9//HHZ2ubNm5PrPvHE\nE8n62bNna+oJrY89PxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EFWacv6urq656PQ4dOpSs7969O1kf\nGRlJ1lPfuR8eHk6ui7jY8wNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUObu6QXM5kvaJmmOpFFJ/e7+\nQzN7TNJqSX/NFl3n7r+q8FrpjQGom7tbNctVE/65kua6+wEzmynpDUn3SHpA0hl3f6rapgg/0HjV\nhr/iFX7uXpJUyu6fNrPDkor96RoAdZvUOb+ZLZT0NUkD2VO9ZvZ7M9tiZleVWafHzAbNbLCuTgHk\nquJh/98WNPuipFclrXf3581stqQPJLmkH2js1OChCq/BYT/QYLmd80uSmX1B0m5Je9x90wT1hZJ2\nu/vSCq9D+IEGqzb8FQ/7zcwk/UTS4fHBzz4IvKBL0juTbRJAcar5tH+5pN9IeltjQ32StE5St6R2\njR32H5X03ezDwdRrsecHGizXw/68EH6g8XI77AcwNRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK\n8ANBEX4gKMIPBEX4gaAIPxAU4QeCavYU3R9I+r9xj7+UPdeKWrW3Vu1Lorda5dnbP1a7YFO/z/+5\njZsNuntHYQ0ktGpvrdqXRG+1Kqo3DvuBoAg/EFTR4e8vePsprdpbq/Yl0VutCumt0HN+AMUpes8P\noCCFhN/M7jSzI2b2rpk9WkQP5ZjZUTN728wOFj3FWDYN2gkze2fcc21m9msz+1N2O+E0aQX19piZ\nvZ+9dwfN7F8L6m2+mb1sZofN7A9m9kj2fKHvXaKvQt63ph/2m9k0SX+U1ClpSNLrkrrd/VBTGynD\nzI5K6nD3wseEzexfJJ2RtO3CbEhm9h+STrr7huwP51Xu/u8t0ttjmuTMzQ3qrdzM0t9Rge9dnjNe\n56GIPf8ySe+6+3vufk7SLyStLKCPlufu+yWdvOjplZK2Zve3auw/T9OV6a0luHvJ3Q9k909LujCz\ndKHvXaKvQhQR/nmS/jLu8ZBaa8pvl7TXzN4ws56im5nA7AszI2W3Vxfcz8UqztzcTBfNLN0y710t\nM17nrYjwTzSbSCsNOXzD3f9Z0l2S1mSHt6jOjyQt1tg0biVJG4tsJptZ+jlJ33P3U0X2Mt4EfRXy\nvhUR/iFJ88c9/rKkYwX0MSF3P5bdnpD0gsZOU1rJ8QuTpGa3Jwru52/c/bi7n3f3UUk/VoHvXTaz\n9HOSfubuz2dPF/7eTdRXUe9bEeF/XdI1ZvYVM5su6duSdhXQx+eY2YzsgxiZ2QxJ31TrzT68S9Kq\n7P4qSTsL7OXvtMrMzeVmllbB712rzXhdyEU+2VDGf0maJmmLu69vehMTMLNFGtvbS2PfePx5kb2Z\n2XZJt2rsW1/HJX1f0n9L+qWkBZL+LOlb7t70D97K9HarJjlzc4N6Kzez9IAKfO/ynPE6l364wg+I\niSv8gKAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8E9f/Ex0YKZYOZcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2f23f9fa978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Show a templete of digits.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "\n",
    "\n",
    "img = np.reshape(trainData[0],(28,28))\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap=cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addBias(data):\n",
    "    bias = np.ones(len(data))[:, np.newaxis]\n",
    "    biasData = np.concatenate((bias,data), axis=1)\n",
    "    return biasData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process the range of pixel value to [-1..1]\n",
    "xTrain = trainData / 127.5 - 1\n",
    "yTrain = trainLabel\n",
    "\n",
    "xTest = testData / 127.5 - 1\n",
    "yTest = testLabel\n",
    "\n",
    "# tack on a \"1\" at the beginning for a bias parameter\n",
    "xTrain = addBias(xTrain)\n",
    "xTest = addBias(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 64)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ij = np.zeros((len(xTrain[0]), 64))   #(featuresNum, unitNum)\n",
    "w_jk = np.zeros((65, 10))   #(unitNum, classNum)\n",
    "w_ij.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1., -1., ..., -1., -1., -1.],\n",
       "       [ 1., -1., -1., ..., -1., -1., -1.],\n",
       "       [ 1., -1., -1., ..., -1., -1., -1.],\n",
       "       ..., \n",
       "       [ 1., -1., -1., ..., -1., -1., -1.],\n",
       "       [ 1., -1., -1., ..., -1., -1., -1.],\n",
       "       [ 1., -1., -1., ..., -1., -1., -1.]])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrainB = xTrain[:10]\n",
    "xTrainB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainBMean = np.mean(xTrainB, axis = 0)[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrainB - xTrainBMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def calculateLoss(data, label, w1, w2, lam):\n",
    "    a_j = data.dot(w1)\n",
    "    ga_j = sigmoid(a_j)\n",
    "    a_k = addBias(ga_j).dot(w2)\n",
    "    ga_k = np.exp(a_k) / np.sum(np.exp(a_k), axis=1)[:, np.newaxis]\n",
    "    E = -1.0 * np.mean(label * np.log(ga_k))\n",
    "    #C = np.sum(np.power(weight, 2))\n",
    "    #loss = E + lam * C\n",
    "    loss = E\n",
    "    return loss\n",
    "\n",
    "def oneHiddenLayer(trainData, trainLabel, lr = 1e-1, maxIter = 500, T = 2e8, lam = 1e-3, batchSize = 1, unitNum = 64):\n",
    "    classNum = len(np.unique(trainLabel))\n",
    "    trainLabel = oneHot(trainLabel)\n",
    "    trainD, validD, trainL, validL = train_test_split(trainData, trainLabel, test_size=0.2, random_state=42)    \n",
    "    w_ij = np.zeros((len(trainD[0]), unitNum))\n",
    "    w_jk = np.zeros((unitNum + 1, classNum))\n",
    "    index = np.arange(len(trainD))\n",
    "    it = 0    \n",
    "    while it <= maxIter:\n",
    "        np.random.shuffle(index)\n",
    "        lr1 = lr / (1.0 + it / T)\n",
    "        # Gradient Descent\n",
    "        for startInd in range(0, len(trainData) - batchSize + 1, batchSize):\n",
    "            trainDBatch = trainD[index[startInd : startInd + batchSize]]\n",
    "            #trainDMean = np.mean(trainDBatch, axis = 0)[np.newaxis, :]\n",
    "            #trainDBatch = trainDBatch - trainDMean\n",
    "            trainLBatch = trainL[index[startInd : startInd + batchSize]]\n",
    "            a_j = trainDBatch.dot(w_ij)   #(batchSize, unitNum) \n",
    "\n",
    "            ga_j = sigmoid(a_j)   #(batchSize, unitNum)\n",
    "            ga_j = addBias(ga_j) \n",
    "            \n",
    "            a_k = ga_j.dot(w_jk)   #(batchSize, classNum)\n",
    "\n",
    "            ga_k = np.exp(a_k) / np.sum(np.exp(a_k), axis=1)[:, np.newaxis]   #(batchSize, classNum)\n",
    "\n",
    "            derGa_j = ga_j * (1 - ga_j)   #(batchSize, unitNum)\n",
    "\n",
    "            delta_k = trainLBatch - ga_k   #(batchSize, classNum)\n",
    "            \n",
    "            delta_j = (derGa_j * (delta_k.dot(w_jk.T)))[:,1:]   #(batchSize, unitNum)\n",
    "            \n",
    "            w_ij = w_ij + lr1 * trainDBatch.T.dot(delta_j) / batchSize   #(featuresNum, unitNum)\n",
    "            \n",
    "            w_jk = w_jk + lr1 * ga_j.T.dot(delta_k) / batchSize   #(unitNum, classNum)\n",
    "\n",
    "        lossV = calculateLoss(validD, validL, w_ij, w_jk, lam)\n",
    "        if it % 10 == 0:\n",
    "            print (lossV)\n",
    "        it += 1\n",
    "    print ('done')\n",
    "    return w_ij, w_jk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.195805614158\n",
      "0.0616952183605\n",
      "0.0524029124967\n",
      "0.0621871636785\n",
      "0.0474895767682\n",
      "0.0549403138559\n",
      "0.0503692556052\n",
      "0.0466224717983\n",
      "0.0467673228287\n",
      "0.0492242556456\n",
      "0.0459807183656\n",
      "0.0474695789354\n",
      "0.0472149326568\n",
      "0.0471399661684\n",
      "0.0460382917607\n",
      "0.0458683596313\n",
      "0.0487997570849\n",
      "0.0468022916045\n",
      "0.046905221414\n",
      "0.0621152997349\n",
      "0.0462132350224\n",
      "0.0466730060055\n",
      "0.0482347907558\n",
      "0.0467715446366\n",
      "0.0484416885629\n",
      "0.0473079145111\n",
      "0.0479059198034\n",
      "0.0470280730662\n",
      "0.047396530812\n",
      "0.0478720643844\n",
      "0.0465603973583\n",
      "0.0488019356052\n",
      "0.0474081171904\n",
      "0.0498292057308\n",
      "0.0477904384944\n",
      "0.0480305974339\n",
      "0.0464173953531\n",
      "0.0487484730229\n",
      "0.0475925233904\n",
      "0.0461765146641\n",
      "0.0470829614626\n",
      "0.0483541530714\n",
      "0.0462511387104\n",
      "0.0464873505135\n",
      "0.0472668769964\n",
      "0.0468797990181\n",
      "0.0484376133475\n",
      "0.0467987769809\n",
      "0.050241804842\n",
      "0.0472704560286\n",
      "0.0471059663834\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "w_ij, w_jk = oneHiddenLayer(xTrain, yTrain, lr = 1e-0, batchSize = int(len(xTrain) / 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pecisionRate(data, label, w1, w2):\n",
    "    correct = 0\n",
    "    label = oneHot(label)\n",
    "    for i in range(len(data)):\n",
    "        a_j = data[i][np.newaxis,:].dot(w1)\n",
    "        ga_j = sigmoid(a_j)\n",
    "        a_k = addBias(ga_j).dot(w2)\n",
    "        ga_k = (np.exp(a_k) / np.sum(np.exp(a_k), axis=1)[:, np.newaxis])[0]\n",
    "        prediction = np.where(ga_k == np.max(ga_k))[0][0]\n",
    "        if label[i, prediction] == 1:\n",
    "            correct += 1\n",
    "    rate = 1.0 * correct / len(label)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8692"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = pecisionRate(xTest, yTest, w_ij, w_jk)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_j = xTest[0][np.newaxis,:].dot(w_ij)\n",
    "a_j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   8.69569171e-01,\n",
       "          8.69569171e-01,   8.69569171e-01,   4.46589193e-21,\n",
       "          9.99520966e-01,   9.99520966e-01,   1.86084491e-08,\n",
       "          4.90624908e-06]])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga_j = sigmoid(a_j)\n",
    "ga_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.30828055,   3.12673913,   6.84109744,   6.39412778,\n",
       "         -9.56782482,  -4.07082743, -10.98520887,  11.85729285,\n",
       "         -1.38986683,   2.1027513 ]])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_k = addBias(ga_j).dot(w_jk)\n",
    "a_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.43170612e-08,   1.59800553e-04,   6.55697608e-03,\n",
       "         4.19360132e-03,   4.90229232e-10,   1.19595500e-07,\n",
       "         1.18805660e-10,   9.89030268e-01,   1.74598792e-06,\n",
       "         5.73939371e-05])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ga_k = (np.exp(a_k) / np.sum(np.exp(a_k), axis=1)[:, np.newaxis])[0]\n",
    "ga_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.where(ga_k == np.max(ga_k))[0][0]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7], dtype=uint8)"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batchSize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-309-a8784d30bb26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mdelta_j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mderGa_j\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdelta_k\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_jk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;31m#(batchSize, unitNum)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mw_ij\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_ij\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mxTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_j\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatchSize\u001b[0m   \u001b[1;31m#(featuresNum, unitNum)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mw_jk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw_jk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mga_j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_k\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatchSize\u001b[0m   \u001b[1;31m#(unitNum, classNum)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batchSize' is not defined"
     ]
    }
   ],
   "source": [
    "a_j = xTrain[:5].dot(w_ij)   #(batchSize, unitNum) \n",
    "\n",
    "ga_j = sigmoid(a_j)   #(batchSize, unitNum)\n",
    "\n",
    "a_k = addBias(ga_j).dot(w_jk)   #(batchSize, classNum)\n",
    "\n",
    "ga_k = np.exp(a_k) / np.sum(np.exp(a_k), axis=1)[:, np.newaxis]   #(batchSize, classNum)\n",
    "\n",
    "derGa_j = ga_j * (1 - ga_j)   #(batchSize, unitNum)\n",
    "\n",
    "delta_k = yTrHot[:5] - ga_k   #(batchSize, classNum)\n",
    "\n",
    "delta_j = derGa_j * (delta_k.dot(w_jk.T))[:,1:]   #(batchSize, unitNum)\n",
    "\n",
    "w_ij = w_ij + 1e-3 * xTrain[:5].T.dot(delta_j) / batchSize   #(featuresNum, unitNum)\n",
    "\n",
    "w_jk = w_jk + 1e-3 * ga_j.T.dot(delta_k) / batchSize   #(unitNum, classNum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression via Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define data extraction function\n",
    "def dataExtract(data, label, target1, target2):\n",
    "    indexTg1 = np.where(label == target1)[0]\n",
    "    dataTg1 = data[indexTg1]\n",
    "    labelTg1 = np.ones(len(indexTg1))[:,np.newaxis]\n",
    "    indexTg2 = np.where(label == target2)[0]\n",
    "    dataTg2 = data[indexTg2]\n",
    "    labelTg2 = np.zeros(len(indexTg2))[:,np.newaxis]\n",
    "    dataEx = np.concatenate((dataTg1,dataTg2), axis = 0)\n",
    "    labelEx = np.concatenate((labelTg1,labelTg2), axis = 0)\n",
    "    return dataEx, labelEx\n",
    "\n",
    "xTrain28, yTrain28 = dataExtract(xTrain, yTrain, 2, 8)\n",
    "xTest28, yTest28 = dataExtract(xTest, yTest, 2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "def calculateLossLR(data, label, weight, lam):\n",
    "    E = -1.0 * np.mean(label * np.log(sigmoid(np.dot(data, weight)))\\\n",
    "                         + (1 - label) * np.log(1 - sigmoid(np.dot(data, weight))))\n",
    "    C = np.sum(np.power(weight, 2))\n",
    "    loss = E + lam * C\n",
    "    return loss\n",
    "\n",
    "def logisticRegression(trainData, trainLabel, lr = 1e-3, maxIter = 500, T = 2e8, lam = 1e-3, batchSize = 1):\n",
    "    trainD, validD, trainL, validL = train_test_split(trainData, trainLabel, test_size=0.1, random_state=42)        \n",
    "    w_0 = np.zeros((len(trainD[0]), 1))\n",
    "    it = 0\n",
    "    while it <= maxIter:\n",
    "        lr1 = lr / (1.0 + it / T)\n",
    "        for startInd in range(0, len(trainData) - batchSize + 1, batchSize):\n",
    "            trainDBatch = trainD[startInd : startInd + batchSize]\n",
    "            trainLBatch = trainL[startInd : startInd + batchSize]\n",
    "            derE = np.mean((sigmoid(np.dot(trainDBatch, w_0)) - trainLBatch) \\\n",
    "                          * trainDBatch, axis = 0)[:, np.newaxis]\n",
    "            derC = 2.0 * np.sum(w_0)\n",
    "            derJ = derE + lam * derC\n",
    "            w_0 -= lr1 * derJ\n",
    "        loss = calculateLossLR(validD, validL, w_0, lam)\n",
    "        if it % 100 == 0:\n",
    "            print(loss)\n",
    "        it += 1\n",
    "    return w_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.429911105598\n",
      "0.0844853103409\n",
      "0.0793186686404\n",
      "0.0777809370218\n",
      "0.0771583228958\n",
      "0.0768908884076\n",
      "0.0767985002534\n",
      "0.0768065973458\n",
      "0.0768783603732\n",
      "0.0769931182047\n",
      "0.0771381333187\n"
     ]
    }
   ],
   "source": [
    "weight = logisticRegression(xTrain28, yTrain28, 1.5e-2, 1000, lam = 1e-4, batchSize = int(len(xTrain28) / 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98055832502492524"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pecisionRate(data, label, weight):\n",
    "    pred = sigmoid(np.dot(data, weight))\n",
    "    pred[pred >= 0.5] = 1\n",
    "    pred[pred < 0.5] = 0\n",
    "    rate = 1.0 * np.sum([a == b for (a, b) in zip(pred, label)]) / len(label)\n",
    "    return rate\n",
    "\n",
    "acc = pecisionRate(xTest28, yTest28, weight)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def calculateLoss(data, label, weight, lam):\n",
    "    a = data.dot(weight)\n",
    "    y = np.exp(a) / np.sum(np.exp(a), axis=1)[:, np.newaxis]\n",
    "    E = -1.0 * np.mean(label * np.log(y))\n",
    "    C = np.sum(np.power(weight, 2))\n",
    "    loss = E + lam * C\n",
    "    return loss\n",
    "\n",
    "def oneHot(label):\n",
    "    labelHot = np.zeros((len(label),len(np.unique(label))))\n",
    "    for i in range(len(label)):\n",
    "        labelHot[i,label[i]] = 1\n",
    "    return labelHot\n",
    "\n",
    "def softmaxRegression(trainData, trainLabel, lr = 1e-4, maxIter = 500, T = 2e8, lam = 1e-3, batchSize = 1):\n",
    "    classNum = len(np.unique(trainLabel))\n",
    "    trainLabel = oneHot(trainLabel)\n",
    "    trainD, validD, trainL, validL = train_test_split(trainData, trainLabel, test_size=0.1, random_state=42)    \n",
    "    w_0 = np.zeros((len(trainData[0]), classNum))\n",
    "    it = 0    \n",
    "    while it <= maxIter:\n",
    "        lr1 = lr / (1.0 + it / T)\n",
    "        # Gradient Descent\n",
    "        for startInd in range(0, len(trainData) - batchSize + 1, batchSize):\n",
    "            trainDBatch = trainD[startInd : startInd + batchSize]\n",
    "            trainLBatch = trainL[startInd : startInd + batchSize]\n",
    "            a = trainDBatch.dot(w_0)\n",
    "            y = np.exp(a) / np.sum(np.exp(a), axis=1)[:, np.newaxis]\n",
    "            y = y - trainLBatch        \n",
    "            derE = 1.0 * trainDBatch.T.dot(y) / batchSize\n",
    "            derC = 2.0 * np.sum(w_0)\n",
    "            derJ = derE + lam * derC\n",
    "            w_0 -= lr1 * derJ\n",
    "        lossV = calculateLoss(validD, validL, w_0, lam)\n",
    "        if it % 100 == 0:\n",
    "            print (lossV)\n",
    "        it += 1\n",
    "    print ('done')\n",
    "    return w_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44450540388\n",
      "0.0337977770761\n",
      "0.033913156151\n",
      "0.0446888522617\n",
      "0.035740750449\n",
      "0.0368835198606\n",
      "0.0660028793269\n",
      "0.0381598880233\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "weight = softmaxRegression(xTrain, yTrain, 1.75e-1, 700, lam = 1e-4, batchSize = int(len(xTrain) / 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pecisionRate(data, label, weight):\n",
    "    correct = 0\n",
    "    label = oneHot(label)\n",
    "    for i in range(len(data)):\n",
    "        a = data[i][np.newaxis,:].dot(weight)\n",
    "        y = (np.exp(a) / np.sum(np.exp(a)))[0]\n",
    "        prediction = np.where(y == np.max(y))[0][0]\n",
    "        if label[i, prediction] == 1:\n",
    "            correct += 1\n",
    "    rate = 1.0 * correct / len(label)\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9239"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTeHot = oneHot(yTest)\n",
    "acc = pecisionRate(xTest, yTest, weight)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yTrHot = oneHot(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
